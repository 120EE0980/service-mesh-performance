# Common service mesh benchmark configuration and result summary
smps_version: smps/v0.1.0

# All times in UTC
start_time: "2019-01-01T12:00:00Z"
end_time: "2019-01-01T12:00:00Z"

# mesh details
mesh_build: "xx.xx.xx"
proxy_build: "xx.xx.xx"

# benchmark identifier
## optional: performance experiment group identifier (collect results of the same configuration under a common id)
exp_group_uuid: "uuid_string"

## individual performance experiment unique identifier
exp_uuid: "uuid_string"

# experiments have different standard mesh profiles
profile: "full, ingress_only, telemetry, encryption"

# location of posted results
details_uri: "https://xxx.xxx"

# URL of the service that is getting tested (getting load thrown at it)
endpoint_url: "https://xxx.xxx"

# Test labels (Helpful for categorizing test results)
labels:
  version: v1
  app: project-xxx
  environment: production
  tool: wrk2

# environment: resource details and versions
env : 
  kubernetes: gke-1.10.5
  node_count: 5
    node: 
      type: "n32"
      cores: 32
      mem_mb: 4096

config:
  mesh_policy_enabled: true
  mesh_telemetry_enabled: true
  mtls_enabled: true
  proxy_concurrency: 0

client:
  # Is the client inside or outside the mesh
  # A client inside the mesh will not use ingress_gateway
  internal: false

  protocol: "http | tcp | grpc"

  connections: 20
  rps: 1000

  # latency histogram in ms and average
  latencies_ms:  
    min: 4
    average: 20
    p50: 22
    p90: 29
    p99: 40
    max: 45

metrics:
  ingress_gateway:
    count: 5
    # cpu in mCores
    cpu_mCores: 2010
    # memory in MB
    mem_mb: 350

    # traffic sent thru ingress gateway
    rps: 1000

    # Total bytes sent thru ingress
    bps: 89000

  sidecars:
    count: 20
    # cpu used by all sidecars except ingress / egress
    cpu_mCores: 3000

    mem_mb: 600

    # Total rps traversing all sidecars
    rps: 8000

    # Total bytes sent thru sidecars
    bps: 213004

  mesh_telemetry:
    count: 20
    # cpu used by all mesh_telemetry pods
    cpu_mCores: 3000

    mem_mb: 600

    # Total rps traversing all proxies (sidecars+ingress+egress)
    rps: 8000

  mesh_policy:
    count: 20
    # cpu used by all mesh_policy pods
    cpu_mCores: 3000

    mem_mb: 600

    # Total rps traversing all proxies (sidecars+ingress+egress)
    rps: 8000
    
    cache_hit_rate: 99


  mesh_control_plane:
    count: 20
    # cpu used by mesh_pilot pods
    cpu_mCores: 3000

    mem_mb: 600

    endpoints: 200
    services: 400   # services + service entries
    sidecars: 120
    virtual_services: 50
    destination_rules: 55
    
    # how long does it take a listener change to propagate to 90% proxies
    lds_latency_ms: 1020

    # how long does it take for cluster change to propagate to 90% proxies
    cds_latency_ms: 1020

  # additional individual workloads should be listed here
  # Only sidecar metrics are measured
  individual_workload_1:
    name: "fortioserver"
    count: 2
    cpu_mCores: 55
    mem_mb: 100
